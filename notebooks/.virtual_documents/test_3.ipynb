import os
import uuid
from google.cloud import storage

os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '../red-freedom-426709-a7-6904e9a53b27.json'
storage_client = storage.Client()
bucket_name = 'datalakes-ing3'

def upload_images_from_folder(folder_path):

    uuids = []

    if folder_path and os.path.isdir(folder_path):
        for filename in os.listdir(folder_path):
            img_path = os.path.join(folder_path, filename)
            unique_id = str(uuid.uuid4())
            uuids.append(unique_id)

            bucket = storage_client.bucket(bucket_name)
            destination_blob_name = f'0_raw/{unique_id}'
            blob = bucket.blob(destination_blob_name)
            blob.upload_from_filename(img_path)
            print(f"File {img_path} uploaded to {destination_blob_name}.")

    with open('../UUIDs.txt', 'w') as file:
        for unique_id in uuids:
            file.write(f"{unique_id}\n")

    return uuids


from google.cloud import storage
from rembg import new_session, remove
from pathlib import Path
from PIL import Image
import io
import os

os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '../red-freedom-426709-a7-6904e9a53b27.json'
storage_client = storage.Client()
bucket_name = 'datalakes-ing3'

session = new_session()
bucket = storage_client.bucket(bucket_name)

input_dir = '0_raw/'
output_dir = '1_staging/'

def background_removal(uuid_list):
    blobs = storage_client.list_blobs(bucket_name, prefix=input_dir)

    for blob in blobs:
        if Path(blob.name).stem in uuid_list:
            input_content = blob.download_as_bytes()
            output_content = remove(input_content, session=session)
            output_image = Image.open(io.BytesIO(output_content))
            
            if output_image.mode == 'RGBA':
                output_image = output_image.convert('RGB')
            
            output_image = output_image.resize((224, 224), Image.LANCZOS)
            
            output_buffer = io.BytesIO()
            output_image.save(output_buffer, format='JPEG')
            output_buffer.seek(0)
            
            filename = Path(blob.name).stem + '.nbg.jpg'
            output_blob_name = f'{output_dir}{filename}'

            output_blob = bucket.blob(output_blob_name)
            output_blob.upload_from_file(output_buffer, content_type='image/jpeg')


from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input
from tensorflow.keras.models import Model
from keras.preprocessing import image

import numpy as np
from PIL import Image
import io
from pinecone import Pinecone
from google.cloud import storage
import os
from pathlib import Path
from dotenv import find_dotenv, load_dotenv

os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '../red-freedom-426709-a7-6904e9a53b27.json'

storage_client = storage.Client()
bucket_name = 'datalakes-ing3'
base_model = VGG16(weights='imagenet', include_top=True)
model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc2').output)

dotenv_path = find_dotenv("../keys.env", raise_error_if_not_found=True, usecwd=True)
load_dotenv(dotenv_path, override=True)

pinecone = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))
index = pinecone.Index('datalakes-ing3-curated')

def vectorize_images_from_uuid(uuid_list, bucket_name='datalakes-ing3'):

    modified_uuid_list = [f"{uuid}.nbg" for uuid in uuid_list]

    bucket = storage_client.bucket(bucket_name)

    def extract_features(image_bytes):
        img = Image.open(io.BytesIO(image_bytes))
        img = img.resize((224, 224))
        img_data = np.expand_dims(image.img_to_array(img), axis=0)
        img_data = preprocess_input(img_data)
        
        vgg16_feature = model.predict(img_data)
        flat_feature = vgg16_feature.flatten()
        return flat_feature

    blobs = storage_client.list_blobs(bucket_name, prefix='1_staging/', delimiter='/')

    for blob in blobs:
        file_uuid = Path(blob.name).stem.split('.')[0]
        print(file_uuid, uuid_list)
        if file_uuid in uuid_list:
            try:
                img_content = blob.download_as_bytes()
                features = extract_features(img_content)
                features /= np.linalg.norm(features)
                index.upsert([(blob.name, features)])
                print(f"Processed {blob.name}")

            except Exception as e:
                print(f"Error processing {blob.name}: {e}")


uuids = upload_images_from_folder("E:\AI\projets\projet_final_datalakes\data-to-add\leo")


background_removal(uuids)


vectorize_images_from_uuid(uuids)






